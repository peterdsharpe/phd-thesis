\chapter{Design Optimization Rules of Thumb}

\begin{quote}
    \emph{This appendix section includes content adapted from the author's Master's thesis with new insights} \cite{sharpe_aerosandbox_2021}.
\end{quote}

Here, we share several general guidelines for design optimization that have been developed across various aircraft development programs. These are especially applicable to the conceptual design of engineering systems, but can be considered in any part of the design cycle:

\begin{enumerate}
    \item \textbf{Engineering time is often part of the objective function.}
    \begin{enumerate}
        \item As the saying goes, 80\% of the results come from the first 20\% of the work: low-fidelity models go exceptionally far.
        \item Make convenient modeling assumptions often and judiciously. (Of course, track these assumptions and revise models if needed.) It is often much more time-efficient to start low-fidelity and only increase fidelity as needed.
        \item Identify sensitive models, requirements, and assumptions early, and estimate the uncertainty associated with each of these. The vast majority of engineering time should be spent on refining these sensitive elements.
    \end{enumerate}
    \item \textbf{Modeling ``wide'' rather than ``deep'' often yields more useful design insight.}
    \begin{enumerate}
        \item Generally, the conceptual design studies that are the most practical, useful, and robust are those that model a vast number of disciplines at low fidelity, rather than those that model one or two disciplines at a high fidelity. Value simple physics models, and let the complexity come naturally from emergent cross-disciplinary behavior.
        \item In instances where high fidelity is truly required, consider surrogate modeling and reduced-order modeling. It is of paramount importance that the optimization problem can be solved in seconds or minutes. If this is not the case, interactive design becomes prohibitively tedious, and extracting engineering intuition becomes difficult.
    \end{enumerate}
    \item \textbf{When building analysis models, be deliberate about where uncertainty is tracked.}
    \begin{enumerate}
        \item Model outputs should reflect your ``maximum likelihood estimator'' of the true value, and be unbiased. This is often \emph{not} the raw output of a first-principles physics model\footnote{For example, a wetted-area drag-buildup analysis that is suspected of biasing low should have a correction factor—and this factor does not count as margin.}. A good rough litmus test for biased outputs is whether one would be more surprised to find the model overpredicting or underpredicting the true value. If one of these would be more surprising, the model should have a correction factor.
        \item Account for margin separately and explicitly, and usually in key top-level closure loops (weight, drag, power, etc.). Remember that margin is performance beyond the \emph{limit} case, not the \emph{baseline} case\footnote{For example, in aircraft design, thrust margin is the excess beyond what is needed for \emph{climb} (possibly with one engine inoperative), not \emph{cruise}. If performance is ever intended to be used during limit-case operation of the system, this cannot be counted as margin.}.
    \end{enumerate}
    \item \textbf{Do not blindly trust an optimizer.}
    \begin{enumerate}
        \item An optimizer only solves the problem that you give it—and this is in the ideal case! Often, it is easy to forget constraints that seem intuitively obvious.
        \item When one is developing models, one's relationship with the optimizer is adversarial. Models should extrapolate sensibly and generally be parsimonious—errors will be \emph{actively} exploited. Because of this, it takes much more finesse to write an analysis tool that is amenable to optimization than one that simply solves the analysis problem.
        \item Consider robustness early. In nature\footnote{Mother Nature being arguably history's most successful optimizer}, optima are usually not near extremes.
    \end{enumerate}
    \item \textbf{Strange results are nearly always the ``right solution to the wrong problem'', rather than the ``wrong solution to the right problem''.}
    \begin{enumerate}
        \item If a strange solution, error, or indication of infeasibility or unboundedness is reported when this was not expected, this often indicates an error in problem formulation. Most of the time, the error is in the constraints: missing or redundant constraints, erroneous mathematical transcription of design intent, overly tight bounds constraints, etc.
        \item In frameworks using gradient-based optimizers, user-specified models that mathematically violate $C^1$-continuity are another frequent source of non-convergence.
        \item Another common cause of strange results is high-dimensional design problems that optimize around a single operating point \cite{drela_pros_1998}. This can lead to extremely brittle designs that ostensibly perform well in a narrow region around the design point but poorly elsewhere (and in practice).
        \item If initial guesses or problem scales are off by many orders of magnitude, this can also cause slow convergence\footnote{However, typically these factors will not affect the value of the optimum, if it is found. This is true if a) an optimizer like IPOPT is used, which only terminates when KKT-like local optimality conditions are satisfied, and b) the problem is not multimodal enough that convergence to a different local minimum is a concern.}. Strong nonconvexities (e.g., a model interpolating noisy data) can also cause problems.
    \end{enumerate}
    \item \textbf{Build models incrementally, and track changes.}
    \begin{enumerate}
        \item Resist the tendency to build a large design tool from the start ``in one fell swoop'', without testing the code along the way. Doing so makes it enormously difficult to debug when the problem formulation is inevitably found to be flawed. Instead, build a simple minimum-viable design tool first, where the only constraints are high-level design closure\footnote{Usually, this involves weight/lift, thrust/drag, and power closures.}. (In other words, start by just performing basic napkin-math sizing, not optimization.) Then, add new variables, constraints, and analyses incrementally, testing the code with each new feature.
        \item Use a version control system (e.g., Git) to track this iterative development of the problem formulation. This allows rapid identification of when and where a mistake was introduced.
    \end{enumerate}
    \item \textbf{Optimization is just one tool in the design toolbox.}
    \begin{enumerate}
        \item An optimizer will answer sizing questions posed by an engineer, but it will not ask new questions on its own or sanity-check these results.
        \item Resist the urge to justify design decisions with ``because the optimizer said so''. The true goal of design optimization is less to arrive at the final design, and more to provide a principled way for the engineer to explore and understand the design space (by collapsing its dimensionality\footnote{by projecting from a very high-dimensional original design space to a low-dimensional ``optimal'' manifold, based on some trade-space parameters of interest.}). In most cases, taking the time to understand why the optimizer produced a certain result is more valuable than the result itself.
    \end{enumerate}
\end{enumerate}