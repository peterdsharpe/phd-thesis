\chapter{Design Optimization Rules of Thumb}
\label{appendix:rules-of-thumb}

\begin{quote}
    \emph{This appendix includes content from the author's prior work, revised with new insights} \cite{sharpe_aerosandbox_2021}.
\end{quote}

Here, we share some general advice for practical engineering design optimization that the author has collected across various aircraft development programs. These are especially applicable to the conceptual design of engineering systems, but can be considered in any part of the design cycle:

\begin{enumerate}
    \item \textbf{Engineering time is often part of the objective function.}
    \begin{enumerate}
        \item As the saying goes, 80\% of the results come from the first 20\% of the work: low-fidelity models go exceptionally far.
        \item Make convenient modeling assumptions often and judiciously. (Of course, track these assumptions and revise models if needed.) It is often much more time-efficient to start low-fidelity and only increase fidelity as needed.
        \item Identify early which models, requirements, and assumptions are sensitive, and estimate uncertainties associated with each of these. Allocate the vast majority of engineering time to risk reduction of only these sensitive elements.
        \item Hands-on hardware prototyping of subsystems is one of the most direct ways to reduce risk; consider freezing the design early and often to facilitate this. Be wary about falling into ``analysis paralysis''.
    \end{enumerate}
    \item \textbf{Modeling ``wide'' rather than ``deep'' often yields more useful design insight.}
    \begin{enumerate}
        \item Generally, the conceptual design studies that are the most practical, useful, and robust are those that model a vast number of disciplines at low fidelity, rather than those that model one or two disciplines at a high fidelity. Value simple physics models, and let the complexity come naturally from emergent cross-disciplinary behavior.
        \item In instances where high fidelity is truly required, consider surrogate modeling and reduced-order modeling. It is of paramount importance that the optimization problem can be solved in seconds or minutes. If this is not the case, interactive design becomes prohibitively tedious, and extracting engineering intuition becomes difficult.
    \end{enumerate}
    \item \textbf{When creating a design tool, be principled about how uncertainty is bookkept.}
    \begin{enumerate}
        \item Analysis results used in MDO should be an \emph{unbiased} estimator of the author's belief state about some quantity. This is often \emph{not} the raw output of a first-principles physics model, since these often bias optimistic due to assumptions\footnote{As aerospace examples: a) a RANS CFD study on defeatured geometry that ignores flap track fairings, skin gaps, or protuberances; b) an XFoil analysis that assumes laminar flow beyond a line of leading-edge rivets; or c) a wing weight build-up that neglects expected weight growth during detailed design. These expected biases should be corrected, and these corrections \emph{do not count as margin}.}. A good rough litmus test for biased outputs is to consider whether one would be more surprised to learn that the model was erring high versus low—these should ideally carry roughly equal surprise.
        \item Account for margin explicitly, and usually only in top-level closure loops and parameters (weight, drag, power, load factor, etc.). Remember that margin is performance beyond the \emph{limit} case, not the \emph{baseline} case\footnote{For example, in aircraft design, thrust margin is the excess beyond what is needed for \emph{climb} (possibly with one engine inoperative), not \emph{cruise}. If performance is ever intended to be used during limit-case operation of the system, this cannot be counted as margin.}. Also, corrections for suspected bias do not count as margin: margin is for mitigating unknown unknowns, not known unknowns.
    \end{enumerate}
    \item \textbf{Do not blindly trust an optimizer.}
    \begin{enumerate}
        \item An optimizer solves the problem given to it (ideally!), not necessarily the problem intended. Often, it is easy to forget constraints that seem intuitively obvious.
        \item When one is developing models, one's relationship with the optimizer is adversarial. Models should extrapolate sensibly and generally be parsimonious—errors will be \emph{actively} exploited. Because of this, it takes much more finesse to write an analysis tool that is amenable to optimization than one that merely solves the analysis problem.
        \item Consider robustness early. In nature\footnote{Mother Nature being arguably history's most successful optimizer}, optima are usually not near extremes.
    \end{enumerate}
    \item \textbf{Strange results are nearly always the ``right solution to the wrong problem'', rather than the ``wrong solution to the right problem''.}
    \begin{enumerate}
        \item If a strange solution, error, or indication of infeasibility or unboundedness is reported when this was not expected, this often indicates an error in problem formulation. Look for cunning discrepancies between the spirit and the letter of the design code. Most of the time, the error is in the constraints: missing, redundant, or inadvertently-added constraints are common culprits.
        \item In frameworks using gradient-based optimizers, user-specified models that mathematically violate $C^1$-continuity are another frequent source of non-convergence.
        \item Another common cause of strange results is high-dimensional design problems that optimize around a single operating point \cite{drela_pros_1998}. This can lead to extremely brittle designs that ostensibly perform well in a narrow region around the design point but poorly elsewhere (and in practice).
        \item If initial guesses or problem scales are off by many orders of magnitude, this can also cause slow convergence\footnote{However, typically these factors will not affect the value of the optimum, if it is found. This is true if a) an optimizer like IPOPT is used, which only terminates when KKT-like local optimality conditions are satisfied, and b) the problem is not multimodal enough that convergence to a different local minimum is a concern.}. Strong nonconvexities (e.g., a model interpolating noisy data) can also cause problems.
    \end{enumerate}
    \item \textbf{Build models incrementally, and track changes.}
    \begin{enumerate}
        \item Resist the tendency to build a large design tool from the start ``in one fell swoop'', without testing the code along the way. Doing so makes it enormously difficult to debug when the problem formulation is inevitably found to be flawed. Instead, build a simple minimum-viable design tool first, where the only constraints are high-level design closure\footnote{Usually, this involves weight/lift, thrust/drag, and power closures.}. (In other words, start by just performing basic napkin-math sizing, not optimization.) Then, add new variables, constraints, and analyses incrementally, testing the code with each new feature.
        \item Use a version control system (e.g., Git) to track this iterative development of the problem formulation. This allows rapid identification of when and where a mistake was introduced.
    \end{enumerate}
    \item \textbf{Optimization is just one tool in the design toolbox.}
    \begin{enumerate}
        \item An optimizer will answer sizing questions posed by an engineer, but it will not ask new questions on its own or sanity-check these results.
        \item Resist the urge to justify design decisions with ``because the optimizer said so''. The true goal of design optimization is less to arrive at the final design, and more to provide a way for the engineer to explore and understand the design space (by collapsing its dimensionality in a principled way\footnote{by projecting from a very high-dimensional original design space to a low-dimensional manifold containing an ``optimal'' subset, based on some trade-space parameters of interest.}). In most cases, taking the time to understand why the optimizer produced a certain result is more valuable than the result itself.
    \end{enumerate}
\end{enumerate}