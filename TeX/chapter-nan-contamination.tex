\chapter{Sparsity Tracing via NaN-Propagation}
\label{sec:nan_propagation}

A key challenge in applying code transformations is handling external black-box (i.e., non-traceable) function calls within the analysis graph. While code transformations can apply techniques like automatic differentiation and automated sparsity detection to accelerate traceable code, these efficiency gains are lost when opaque black-box functions are present. Here, gradient calculation methods must invariable fall back to slower techniques, such as finite-differencing or complex-step derivative computation \cite{martins_complexstep_2003}. However, significant speedups are still achievable if the sparsity pattern of dependencies between inputs and outputs can be determined, as this allows Jacobian compression via coloring \cite{gebremedhin_efficient_2009, gebremedhin_what_2005}.

This thesis contribution proposes a novel technique that we call \textit{NaN propagation} to trace these input-output dependencies through black-box functions. The approach exploits the fact that Not-a-Number (NaN) values are universally propagated through most floating-point numerical computations via the IEEE 754 standard. Because most floating-point math libraries since the 1970s follow this standard, this presents a fascinating way to potentially trace sparsity independent of math library or programming language. By systematically contaminating inputs to a black-box function with NaN values and observing which outputs become NaN, the sparsity graph can be reconstructed.

Concretely, the technique proceeds as follows:

\begin{enumerate}
    \item Evaluate the black-box function on an initial input.
    \item Re-evaluate the function with a single input replaced by NaN. Outputs that newly become NaN are downstream of that input.
    \item Repeat for each input to trace all dependencies and (conservatively) compute the sparsity pattern of that function's Jacobian.
\end{enumerate}
This simple procedure yields the complete bipartite graph between inputs and outputs (the \textit{sparsity pattern}). The sparsity pattern enables gradient computation accelerations via simultaneous evaluation, since the sparsity of the Jacobian is known to be a strict subset of the result of this procedure. Crucially, no internal knowledge of the black-box function is required, which in theory compatibility with arbitrary external code.

Further accelerations are possible. One novel proposed acceleration, which we term ``chunking'', seeds multiple adjacent elements of the input vector with NaN values simultaneously. As an example: if pairs of adjacent inputs are seeded, this has the benefit of halving the number of function calls, at the detriment of providing overly-conservative sparsity information\footnote{as one cannot determine which NaN input caused a given output to return NaN, so neither can be ruled out}. This exploits the fact that a conservative sparsity pattern can still provide a significant speedup, depending on the sparsity pattern. It also exploits the fact that human-written code tends to have a high degree of locality\footnote{in other words, humans tend to code Discipline A in its entirety before implementing analyses for Discipline B} (e.g., inputs 10 and 11 are much more likely to share a sparsity pattern than inputs 10 and 100), and hence these chunked sparsity patterns can be surprisingly accurate in practice. Thus, chunking may offer significant practical speedups by exploiting typical problem structure; a computational analogy would be how an $A^*$ graph traversal algorithm outperforms Dijkstra's algorithm in practice despite identical worst-case complexity. This and other heuristics can significantly reduce the number of black-box function evaluations required to trace the sparsity pattern.

There are several subtleties to address for robustness across numerical code. First, some functions may internally handle NaN values via exceptions, preventing contamination and hence precluding this strategy. However, NaN propagation is common in most numerical libraries. Second, conditional logic can complicate tracing, but most libraries propagate NaNs through static conditional statements (i.e., a flattened if-else statement where both branches are fused). Overall, NaN propagation shows promise for automated sparsity detection through common black-box functions. Integrating this technique into the code transformation paradigm could expand its applicability.
